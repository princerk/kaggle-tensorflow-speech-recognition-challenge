{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np # linear algebra\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Convolution2D, Activation\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from subprocess import check_output\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import acoustics\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "\n",
    "def timer():\n",
    "    time_start = [int(time.time())]\n",
    "    time_start_overall = [int(time.time())]\n",
    "\n",
    "    def time_taken():\n",
    "        time_now = int(time.time())\n",
    "        om, os = divmod(time_now - time_start_overall[0], 60)\n",
    "        m, s = divmod(time_now - time_start[0], 60)\n",
    "        time_start[0] = time_now\n",
    "        return ' overall_time: ' + str(om) + 'm' + str(os) + 's' + ' time_from_previous_call: ' + str(m) + 'm' + str(s) + 's'\n",
    "    return time_taken\n",
    "time_taken = timer()\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "unknowns = 'bed bird cat dog eight five four happy house marvin nine one seven sheila six three tree two wow zero'.split()\n",
    "knowns = 'yes no up down left right on off stop go'.split()\n",
    "silence = 'silence'.split()\n",
    "labels = knowns + silence + ['unknown'] \n",
    "num_classes = len(labels)\n",
    "\n",
    "LABEL_TO_FILE_NAMES = {}\n",
    "VALIDATION_LABEL_TO_FILE_NAMES = {}\n",
    "TRAIN_LABEL_TO_FILE_NAMES = {}\n",
    "FILE_TO_LABEL = {}\n",
    "with open('train-88.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        TRAIN_LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        TRAIN_LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        FILE_TO_LABEL[line[0]] = line[1]\n",
    "with open('validation-88.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        VALIDATION_LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        VALIDATION_LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        FILE_TO_LABEL[line[0]] = line[1]\n",
    "\n",
    "\n",
    "def file_to_sample(filename):\n",
    "    samples, _ = librosa.load(filename, sr=sr)\n",
    "    return samples\n",
    "    \n",
    "\n",
    "UNCOLORED_NOISES = []\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/doing_the_dishes.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/dude_miaowing.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/exercise_bike.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/running_tap.wav', sr=sr)[0].tolist()\n",
    "\n",
    "\n",
    "def get_silence():\n",
    "    choice = np.random.choice([0, 1, 2, 4], p=[0.01, 0.10, 0.50, 0.39])\n",
    "    if choice == 0:\n",
    "        return np.zeros((16000))\n",
    "    elif choice == 1:\n",
    "        idx = random.randint(0, len(UNCOLORED_NOISES) - sr)\n",
    "        return np.array(UNCOLORED_NOISES[idx:idx+sr], dtype=np.float32)\n",
    "    elif choice == 3:\n",
    "        return np.array(acoustics.generator.noise(16000, color=np.random.choice(['pink', 'white']))/3, np.float32)\n",
    "    else:\n",
    "        random_silence_file = np.random.choice(LABEL_TO_FILE_NAMES['silence'])\n",
    "        return file_to_sample(random_silence_file)\n",
    "\n",
    "def pad_zeros(samples):\n",
    "    if len(samples) < sr:\n",
    "        diff = sr - len(samples)\n",
    "        diff_div = diff // 2\n",
    "        samples = np.lib.pad(samples, (diff_div, diff - diff_div), 'constant', constant_values = (0, 0))\n",
    "    return samples\n",
    "\n",
    "def pitch_shift(samples, sr=sr):\n",
    "    return librosa.effects.pitch_shift(samples, sr=sr, n_steps=random.randint(1, 5))\n",
    "\n",
    "def get_shuffled_XY(X, Y):\n",
    "    m = X.shape[0]\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "    return shuffled_X, shuffled_Y\n",
    "\n",
    "def time_shift(arr):\n",
    "    num = np.random.uniform(0, 0.10) * len(arr)\n",
    "    num = int(num)\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = 0\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = 0\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result = arr\n",
    "    return np.array(result)\n",
    "        \n",
    "def flip_transform(wave):\n",
    "    if np.random.choice([0, 1]):\n",
    "        return -wave\n",
    "\n",
    "def noise_mix(wave):\n",
    "    if np.random.random() < 0.30:\n",
    "        return wave\n",
    "    noise = get_silence()\n",
    "    noise_limit = random.uniform(0, 0.1)\n",
    "    wave = (1 - noise_limit) * wave + noise_limit * noise\n",
    "    return wave\n",
    "\n",
    "# 72 * 72\n",
    "def get_melspectrogram(samples):\n",
    "    S = librosa.feature.melspectrogram(samples, sr=sr, n_mels=72, hop_length=223, n_fft=512)\n",
    "    spec = librosa.power_to_db(S, ref=np.max)\n",
    "    spec = np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "    return spec\n",
    "\n",
    "# 150 * 150\n",
    "def get_mel_of_150_150(samples):\n",
    "    S = librosa.feature.melspectrogram(samples, sr=sr, n_mels=150, hop_length=107, n_fft=512)\n",
    "    spec = librosa.power_to_db(S, ref=np.max)\n",
    "    spec = np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "    return spec\n",
    "\n",
    "# 197 * 161\n",
    "def log_specgram(audio, sample_rate=16000, window_size=20,\n",
    "                 step_size=15, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    spec = np.log(spec.T.astype(np.float32) + eps)\n",
    "    return np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "\n",
    "def get_transformed_samples(samples):\n",
    "    samples = pad_zeros(samples)\n",
    "    samples = time_shift(samples)\n",
    "    samples = noise_mix(samples)\n",
    "    if np.random.choice([0, 1]):\n",
    "        samples = -samples\n",
    "    stdx = np.std(samples)\n",
    "    if stdx:\n",
    "        sampels = samples / stdx\n",
    "    #samples = flip_transform(samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing train label to filename lengths\n",
      "unknown 70315\n",
      "silence 6591\n",
      "stop 6733\n",
      "on 6561\n",
      "no 6248\n",
      "go 5331\n",
      "off 6456\n",
      "down 5355\n",
      "right 6811\n",
      "up 5944\n",
      "left 6552\n",
      "yes 6221\n",
      "printing validation label to filename lengths\n",
      "unknown 7812\n",
      "silence 732\n",
      "stop 748\n",
      "on 729\n",
      "no 694\n",
      "go 592\n",
      "off 717\n",
      "down 594\n",
      "right 756\n",
      "up 660\n",
      "left 728\n",
      "yes 691\n"
     ]
    }
   ],
   "source": [
    "print('printing train label to filename lengths')\n",
    "for label, filenames in TRAIN_LABEL_TO_FILE_NAMES.items():\n",
    "    print(label, len(filenames))\n",
    "print('printing validation label to filename lengths')\n",
    "for label, filenames in VALIDATION_LABEL_TO_FILE_NAMES.items():\n",
    "    print(label, len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " overall_time: 0m10s time_from_previous_call: 0m10s\n"
     ]
    }
   ],
   "source": [
    "print(time_taken())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def validation_data_generator():\n",
    "    XV = np.zeros((batch_size, 197, 161, 3))\n",
    "    YV = np.zeros((batch_size, num_classes))\n",
    "    while True:\n",
    "        for idx in range(batch_size):\n",
    "            random_label = np.random.choice(labels)\n",
    "            if random_label == 'silence':\n",
    "                samples = get_silence()\n",
    "            else:\n",
    "                random_filename = np.random.choice(VALIDATION_LABEL_TO_FILE_NAMES[random_label])\n",
    "                samples = file_to_sample(random_filename)\n",
    "                samples = get_transformed_samples(samples)\n",
    "            spec = log_specgram(samples)\n",
    "            XV[idx, :, :] = spec\n",
    "            this_Y = [labels.index(random_label)]\n",
    "            this_Y = keras.utils.to_categorical(np.array(this_Y).astype(np.float32), num_classes)\n",
    "            YV[idx, :] = this_Y\n",
    "        yield XV, YV\n",
    "\n",
    "def train_data_generator():\n",
    "    XT = np.zeros((batch_size, 197, 161, 3))\n",
    "    YT = np.zeros((batch_size, num_classes))\n",
    "    while True:\n",
    "        for idx in range(batch_size):\n",
    "            random_label = np.random.choice(labels)\n",
    "            if random_label == 'silence':\n",
    "                samples = get_silence()\n",
    "            else:\n",
    "                random_filename = np.random.choice(TRAIN_LABEL_TO_FILE_NAMES[random_label])\n",
    "                samples = file_to_sample(random_filename)\n",
    "                samples = get_transformed_samples(samples)\n",
    "            spec = log_specgram(samples)\n",
    "            XT[idx, :, :] = spec\n",
    "            this_Y = [labels.index(random_label)]\n",
    "            this_Y = keras.utils.to_categorical(np.array(this_Y).astype(np.float32), num_classes)\n",
    "            YT[idx, :] = this_Y\n",
    "        yield XT, YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.7269 - acc: 0.7946Epoch 00001: val_acc improved from -inf to 0.07930, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-01-0.08-2.50.h5\n",
      "200/200 [==============================] - 253s 1s/step - loss: 0.7263 - acc: 0.7951 - val_loss: 2.5037 - val_acc: 0.0793\n",
      "Epoch 2/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.4504 - acc: 0.9040Epoch 00002: val_acc improved from 0.07930 to 0.08359, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-02-0.08-3.20.h5\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.4502 - acc: 0.9040 - val_loss: 3.2025 - val_acc: 0.0836\n",
      "Epoch 3/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3778 - acc: 0.9261Epoch 00003: val_acc improved from 0.08359 to 0.74883, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-03-0.75-0.97.h5\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.3779 - acc: 0.9261 - val_loss: 0.9738 - val_acc: 0.7488\n",
      "Epoch 4/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3744 - acc: 0.9285Epoch 00004: val_acc improved from 0.74883 to 0.83633, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-04-0.84-0.75.h5\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.3748 - acc: 0.9284 - val_loss: 0.7462 - val_acc: 0.8363\n",
      "Epoch 5/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3521 - acc: 0.9358Epoch 00005: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3527 - acc: 0.9357 - val_loss: 1.2569 - val_acc: 0.6523\n",
      "Epoch 6/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3504 - acc: 0.9348Epoch 00006: val_acc improved from 0.83633 to 0.86172, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-06-0.86-0.67.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3501 - acc: 0.9348 - val_loss: 0.6688 - val_acc: 0.8617\n",
      "Epoch 7/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3380 - acc: 0.9385Epoch 00007: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3375 - acc: 0.9386 - val_loss: 0.9236 - val_acc: 0.7664\n",
      "Epoch 8/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3342 - acc: 0.9397Epoch 00008: val_acc improved from 0.86172 to 0.87969, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-08-0.88-0.64.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3338 - acc: 0.9398 - val_loss: 0.6444 - val_acc: 0.8797\n",
      "Epoch 9/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3021 - acc: 0.9460Epoch 00009: val_acc did not improve\n",
      "200/200 [==============================] - 224s 1s/step - loss: 0.3019 - acc: 0.9460 - val_loss: 0.6262 - val_acc: 0.8762\n",
      "Epoch 10/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3200 - acc: 0.9447Epoch 00010: val_acc improved from 0.87969 to 0.88281, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-10-0.88-0.58.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3202 - acc: 0.9447 - val_loss: 0.5769 - val_acc: 0.8828\n",
      "Epoch 11/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3150 - acc: 0.9458Epoch 00011: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3150 - acc: 0.9458 - val_loss: 0.6118 - val_acc: 0.8828\n",
      "Epoch 12/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3157 - acc: 0.9461Epoch 00012: val_acc improved from 0.88281 to 0.88633, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-12-0.89-0.56.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3159 - acc: 0.9461 - val_loss: 0.5582 - val_acc: 0.8863\n",
      "Epoch 13/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3047 - acc: 0.9481Epoch 00013: val_acc did not improve\n",
      "200/200 [==============================] - 224s 1s/step - loss: 0.3049 - acc: 0.9481 - val_loss: 0.5972 - val_acc: 0.8770\n",
      "Epoch 14/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3212 - acc: 0.9444Epoch 00014: val_acc did not improve\n",
      "200/200 [==============================] - 224s 1s/step - loss: 0.3206 - acc: 0.9445 - val_loss: 0.6579 - val_acc: 0.8594\n",
      "Epoch 15/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3102 - acc: 0.9474Epoch 00015: val_acc did not improve\n",
      "200/200 [==============================] - 224s 1s/step - loss: 0.3099 - acc: 0.9475 - val_loss: 0.7313 - val_acc: 0.8352\n",
      "Epoch 16/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3064 - acc: 0.9480Epoch 00016: val_acc improved from 0.88633 to 0.90703, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-16-0.91-0.50.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3066 - acc: 0.9479 - val_loss: 0.5048 - val_acc: 0.9070\n",
      "Epoch 17/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3234 - acc: 0.9451Epoch 00017: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3230 - acc: 0.9452 - val_loss: 0.5139 - val_acc: 0.9051\n",
      "Epoch 18/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3274 - acc: 0.9448Epoch 00018: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3278 - acc: 0.9446 - val_loss: 0.4953 - val_acc: 0.9031\n",
      "Epoch 19/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3226 - acc: 0.9450Epoch 00019: val_acc improved from 0.90703 to 0.91641, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-19-0.92-0.46.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3228 - acc: 0.9450 - val_loss: 0.4630 - val_acc: 0.9164\n",
      "Epoch 20/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3287 - acc: 0.9444Epoch 00020: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3287 - acc: 0.9444 - val_loss: 0.6723 - val_acc: 0.8484\n",
      "Epoch 21/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3150 - acc: 0.9476Epoch 00021: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3146 - acc: 0.9476 - val_loss: 0.4712 - val_acc: 0.9133\n",
      "Epoch 22/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3148 - acc: 0.9469Epoch 00022: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3147 - acc: 0.9469 - val_loss: 0.5510 - val_acc: 0.8930\n",
      "Epoch 23/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3181 - acc: 0.9473Epoch 00023: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.3180 - acc: 0.9473 - val_loss: 0.6090 - val_acc: 0.8855\n",
      "Epoch 24/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3166 - acc: 0.9476Epoch 00024: val_acc did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 0.00020000000949949026.\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.3172 - acc: 0.9476 - val_loss: 0.5737 - val_acc: 0.8750\n",
      "Epoch 25/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.2843 - acc: 0.9539Epoch 00025: val_acc improved from 0.91641 to 0.92852, saving model to model-pseudo-88-logspec-inception-resnet-v2-0-25-0.93-0.42.h5\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.2841 - acc: 0.9539 - val_loss: 0.4152 - val_acc: 0.9285\n",
      "Epoch 26/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.2900 - acc: 0.9537Epoch 00026: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.2901 - acc: 0.9537 - val_loss: 0.4271 - val_acc: 0.9258\n",
      "Epoch 27/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.2828 - acc: 0.9551Epoch 00027: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.2830 - acc: 0.9551 - val_loss: 0.4111 - val_acc: 0.9281\n",
      "Epoch 28/200\n",
      " 94/200 [=============>................] - ETA: 1:54 - loss: 0.2708 - acc: 0.9569"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "model = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=num_classes)\n",
    "model = keras.applications.xception.Xception(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=num_classes)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "#model = load_model('model-pseudo-88-melspec-inception-resnet-v2-0-114-0.99-0.07.h5')\n",
    "checkpoint = ModelCheckpoint('model-pseudo-88-logspec-inception-resnet-v2-0-{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.h5',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='max')\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-8, verbose=1)\n",
    "callback_list = [checkpoint, reduce_lr]\n",
    "\n",
    "train_generator = train_data_generator()\n",
    "validation_generator = validation_data_generator()\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=200, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callback_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=20,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_taken())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
