{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np # linear algebra\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Convolution2D, Activation\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from subprocess import check_output\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import acoustics\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "\n",
    "def timer():\n",
    "    time_start = [int(time.time())]\n",
    "    time_start_overall = [int(time.time())]\n",
    "\n",
    "    def time_taken():\n",
    "        time_now = int(time.time())\n",
    "        om, os = divmod(time_now - time_start_overall[0], 60)\n",
    "        m, s = divmod(time_now - time_start[0], 60)\n",
    "        time_start[0] = time_now\n",
    "        return ' overall_time: ' + str(om) + 'm' + str(os) + 's' + ' time_from_previous_call: ' + str(m) + 'm' + str(s) + 's'\n",
    "    return time_taken\n",
    "time_taken = timer()\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "unknowns = 'bed bird cat dog eight five four happy house marvin nine one seven sheila six three tree two wow zero'.split()\n",
    "knowns = 'yes no up down left right on off stop go'.split()\n",
    "silence = 'silence'.split()\n",
    "labels = knowns + silence + ['unknown'] \n",
    "num_classes = len(labels)\n",
    "\n",
    "LABEL_TO_FILE_NAMES = {}\n",
    "VALIDATION_LABEL_TO_FILE_NAMES = {}\n",
    "TRAIN_LABEL_TO_FILE_NAMES = {}\n",
    "FILE_TO_LABEL = {}\n",
    "with open('train-88.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        TRAIN_LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        TRAIN_LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        FILE_TO_LABEL[line[0]] = line[1]\n",
    "with open('validation-88.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        VALIDATION_LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        VALIDATION_LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        FILE_TO_LABEL[line[0]] = line[1]\n",
    "\n",
    "\n",
    "def file_to_sample(filename):\n",
    "    samples, _ = librosa.load(filename, sr=sr)\n",
    "    return samples\n",
    "    \n",
    "\n",
    "UNCOLORED_NOISES = []\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/doing_the_dishes.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/dude_miaowing.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/exercise_bike.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/running_tap.wav', sr=sr)[0].tolist()\n",
    "\n",
    "\n",
    "def get_silence():\n",
    "    choice = np.random.choice([0, 1, 2, 4], p=[0.01, 0.10, 0.70, 0.19])\n",
    "    if choice == 0:\n",
    "        return np.zeros((16000))\n",
    "    elif choice == 1:\n",
    "        idx = random.randint(0, len(UNCOLORED_NOISES) - sr)\n",
    "        return np.array(UNCOLORED_NOISES[idx:idx+sr], dtype=np.float32)\n",
    "    elif choice == 3:\n",
    "        return np.array(acoustics.generator.noise(16000, color=np.random.choice(['pink', 'white']))/3, np.float32)\n",
    "    else:\n",
    "        random_silence_file = np.random.choice(LABEL_TO_FILE_NAMES['silence'])\n",
    "        return file_to_sample(random_silence_file)\n",
    "\n",
    "def pad_zeros(samples):\n",
    "    if len(samples) < sr:\n",
    "        diff = sr - len(samples)\n",
    "        diff_div = diff // 2\n",
    "        samples = np.lib.pad(samples, (diff_div, diff - diff_div), 'constant', constant_values = (0, 0))\n",
    "    return samples\n",
    "\n",
    "def pitch_shift(samples, sr=sr):\n",
    "    return librosa.effects.pitch_shift(samples, sr=sr, n_steps=random.randint(1, 5))\n",
    "\n",
    "def get_shuffled_XY(X, Y):\n",
    "    m = X.shape[0]\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "    return shuffled_X, shuffled_Y\n",
    "\n",
    "def time_shift(arr):\n",
    "    num = np.random.uniform(0, 0.2) * len(arr)\n",
    "    num = int(num)\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = 0\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = 0\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result = arr\n",
    "    return np.array(result)\n",
    "        \n",
    "def flip_transform(wave):\n",
    "    if np.random.choice([0, 1]):\n",
    "        return -wave\n",
    "\n",
    "def noise_mix(wave):\n",
    "    noise = get_silence()\n",
    "    noise_limit = random.uniform(0, 0.1)\n",
    "    wave = (1 - noise_limit) * wave + noise_limit * noise\n",
    "    return wave\n",
    "\n",
    "# 72 * 72\n",
    "def get_melspectrogram(samples):\n",
    "    S = librosa.feature.melspectrogram(samples, sr=sr, n_mels=72, hop_length=223, n_fft=512)\n",
    "    spec = librosa.power_to_db(S, ref=np.max)\n",
    "    spec = np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "    return spec\n",
    "\n",
    "# 150 * 150\n",
    "def get_mel_of_150_150(samples):\n",
    "    S = librosa.feature.melspectrogram(samples, sr=sr, n_mels=150, hop_length=107, n_fft=512)\n",
    "    spec = librosa.power_to_db(S, ref=np.max)\n",
    "    spec = np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "    return spec\n",
    "\n",
    "# 197 * 161\n",
    "def log_specgram(audio, sample_rate=16000, window_size=20,\n",
    "                 step_size=15, eps=1e-10):\n",
    "    nperseg = int(round(window_size * sample_rate / 1e3))\n",
    "    noverlap = int(round(step_size * sample_rate / 1e3))\n",
    "    freqs, times, spec = signal.spectrogram(audio,\n",
    "                                    fs=sample_rate,\n",
    "                                    window='hann',\n",
    "                                    nperseg=nperseg,\n",
    "                                    noverlap=noverlap,\n",
    "                                    detrend=False)\n",
    "    spec = np.log(spec.T.astype(np.float32) + eps)\n",
    "    return np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "\n",
    "def get_transformed_samples(samples):\n",
    "    samples = pad_zeros(samples)\n",
    "    samples = time_shift(samples)\n",
    "    samples = noise_mix(samples)\n",
    "    if np.random.choice([0, 1]):\n",
    "        samples = -samples\n",
    "    stdx = np.std(samples)\n",
    "    if stdx:\n",
    "        sampels = samples / stdx\n",
    "    #samples = flip_transform(samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing train label to filename lengths\n",
      "no 6248\n",
      "up 5944\n",
      "right 6811\n",
      "down 5355\n",
      "off 6456\n",
      "go 5331\n",
      "on 6561\n",
      "yes 6221\n",
      "unknown 70315\n",
      "stop 6733\n",
      "left 6552\n",
      "silence 6591\n",
      "printing validation label to filename lengths\n",
      "no 694\n",
      "up 660\n",
      "right 756\n",
      "down 594\n",
      "off 717\n",
      "go 592\n",
      "on 729\n",
      "yes 691\n",
      "unknown 7812\n",
      "stop 748\n",
      "left 728\n",
      "silence 732\n"
     ]
    }
   ],
   "source": [
    "print('printing train label to filename lengths')\n",
    "for label, filenames in TRAIN_LABEL_TO_FILE_NAMES.items():\n",
    "    print(label, len(filenames))\n",
    "print('printing validation label to filename lengths')\n",
    "for label, filenames in VALIDATION_LABEL_TO_FILE_NAMES.items():\n",
    "    print(label, len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " overall_time: 0m23s time_from_previous_call: 0m23s\n"
     ]
    }
   ],
   "source": [
    "print(time_taken())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def validation_data_generator():\n",
    "    XV = np.zeros((batch_size, 150, 150, 3))\n",
    "    YV = np.zeros((batch_size, num_classes))\n",
    "    while True:\n",
    "        for idx in range(batch_size):\n",
    "            random_label = np.random.choice(labels)\n",
    "            if random_label == 'silence':\n",
    "                samples = get_silence()\n",
    "            else:\n",
    "                random_filename = np.random.choice(VALIDATION_LABEL_TO_FILE_NAMES[random_label])\n",
    "                samples = file_to_sample(random_filename)\n",
    "                samples = get_transformed_samples(samples)\n",
    "            spec = get_mel_of_150_150(samples)\n",
    "            XV[idx, :, :] = spec\n",
    "            this_Y = [labels.index(random_label)]\n",
    "            this_Y = keras.utils.to_categorical(np.array(this_Y).astype(np.float32), num_classes)\n",
    "            YV[idx, :] = this_Y\n",
    "        yield XV, YV\n",
    "\n",
    "def train_data_generator():\n",
    "    XT = np.zeros((batch_size, 150, 150, 3))\n",
    "    YT = np.zeros((batch_size, num_classes))\n",
    "    while True:\n",
    "        for idx in range(batch_size):\n",
    "            random_label = np.random.choice(labels)\n",
    "            if random_label == 'silence':\n",
    "                samples = get_silence()\n",
    "            else:\n",
    "                random_filename = np.random.choice(TRAIN_LABEL_TO_FILE_NAMES[random_label])\n",
    "                samples = file_to_sample(random_filename)\n",
    "                samples = get_transformed_samples(samples)\n",
    "            spec = get_mel_of_150_150(samples)\n",
    "            XT[idx, :, :] = spec\n",
    "            this_Y = [labels.index(random_label)]\n",
    "            this_Y = keras.utils.to_categorical(np.array(this_Y).astype(np.float32), num_classes)\n",
    "            YT[idx, :] = this_Y\n",
    "        yield XT, YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.5982 - acc: 0.8105Epoch 00001: val_acc improved from -inf to 0.77695, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-01-0.78-0.72.h5\n",
      "200/200 [==============================] - 567s 3s/step - loss: 0.5963 - acc: 0.8111 - val_loss: 0.7178 - val_acc: 0.7770\n",
      "Epoch 2/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.3038 - acc: 0.9196Epoch 00002: val_acc improved from 0.77695 to 0.90234, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-02-0.90-0.34.h5\n",
      "200/200 [==============================] - 364s 2s/step - loss: 0.3036 - acc: 0.9197 - val_loss: 0.3372 - val_acc: 0.9023\n",
      "Epoch 3/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.2515 - acc: 0.9395Epoch 00003: val_acc improved from 0.90234 to 0.90977, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-03-0.91-0.30.h5\n",
      "200/200 [==============================] - 355s 2s/step - loss: 0.2516 - acc: 0.9396 - val_loss: 0.2972 - val_acc: 0.9098\n",
      "Epoch 4/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.2270 - acc: 0.9459Epoch 00004: val_acc did not improve\n",
      "200/200 [==============================] - 350s 2s/step - loss: 0.2263 - acc: 0.9461 - val_loss: 0.4062 - val_acc: 0.8883\n",
      "Epoch 5/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.2133 - acc: 0.9494Epoch 00005: val_acc improved from 0.90977 to 0.93125, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-05-0.93-0.27.h5\n",
      "200/200 [==============================] - 348s 2s/step - loss: 0.2130 - acc: 0.9495 - val_loss: 0.2654 - val_acc: 0.9313\n",
      "Epoch 6/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1862 - acc: 0.9569Epoch 00006: val_acc did not improve\n",
      "200/200 [==============================] - 346s 2s/step - loss: 0.1864 - acc: 0.9569 - val_loss: 0.3429 - val_acc: 0.9023\n",
      "Epoch 7/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1787 - acc: 0.9596Epoch 00007: val_acc improved from 0.93125 to 0.94141, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-07-0.94-0.23.h5\n",
      "200/200 [==============================] - 342s 2s/step - loss: 0.1785 - acc: 0.9596 - val_loss: 0.2275 - val_acc: 0.9414\n",
      "Epoch 8/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1799 - acc: 0.9594Epoch 00008: val_acc did not improve\n",
      "200/200 [==============================] - 338s 2s/step - loss: 0.1794 - acc: 0.9595 - val_loss: 0.2485 - val_acc: 0.9277\n",
      "Epoch 9/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1626 - acc: 0.9650Epoch 00009: val_acc improved from 0.94141 to 0.95391, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-09-0.95-0.20.h5\n",
      "200/200 [==============================] - 337s 2s/step - loss: 0.1626 - acc: 0.9650 - val_loss: 0.1965 - val_acc: 0.9539\n",
      "Epoch 10/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1737 - acc: 0.9631Epoch 00010: val_acc did not improve\n",
      "200/200 [==============================] - 341s 2s/step - loss: 0.1737 - acc: 0.9630 - val_loss: 0.1922 - val_acc: 0.9437\n",
      "Epoch 11/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1713 - acc: 0.9628Epoch 00011: val_acc did not improve\n",
      "200/200 [==============================] - 333s 2s/step - loss: 0.1714 - acc: 0.9627 - val_loss: 0.2721 - val_acc: 0.9234\n",
      "Epoch 12/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1614 - acc: 0.9638Epoch 00012: val_acc did not improve\n",
      "200/200 [==============================] - 329s 2s/step - loss: 0.1613 - acc: 0.9639 - val_loss: 0.4366 - val_acc: 0.8758\n",
      "Epoch 13/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1615 - acc: 0.9660Epoch 00013: val_acc did not improve\n",
      "200/200 [==============================] - 334s 2s/step - loss: 0.1613 - acc: 0.9661 - val_loss: 0.3585 - val_acc: 0.8996\n",
      "Epoch 14/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1497 - acc: 0.9680Epoch 00014: val_acc did not improve\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.1503 - acc: 0.9678 - val_loss: 0.3147 - val_acc: 0.9090\n",
      "Epoch 15/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1536 - acc: 0.9673Epoch 00015: val_acc did not improve\n",
      "200/200 [==============================] - 332s 2s/step - loss: 0.1536 - acc: 0.9673 - val_loss: 0.1872 - val_acc: 0.9531\n",
      "Epoch 16/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1423 - acc: 0.9695Epoch 00016: val_acc did not improve\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.1422 - acc: 0.9695 - val_loss: 0.2551 - val_acc: 0.9262\n",
      "Epoch 17/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1424 - acc: 0.9701Epoch 00017: val_acc did not improve\n",
      "200/200 [==============================] - 335s 2s/step - loss: 0.1420 - acc: 0.9701 - val_loss: 0.2166 - val_acc: 0.9426\n",
      "Epoch 18/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1474 - acc: 0.9703Epoch 00018: val_acc improved from 0.95391 to 0.96133, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-18-0.96-0.16.h5\n",
      "200/200 [==============================] - 330s 2s/step - loss: 0.1477 - acc: 0.9702 - val_loss: 0.1592 - val_acc: 0.9613\n",
      "Epoch 19/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1423 - acc: 0.9694Epoch 00019: val_acc did not improve\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.1424 - acc: 0.9693 - val_loss: 0.3152 - val_acc: 0.9129\n",
      "Epoch 20/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1527 - acc: 0.9695Epoch 00020: val_acc did not improve\n",
      "200/200 [==============================] - 333s 2s/step - loss: 0.1533 - acc: 0.9694 - val_loss: 0.3034 - val_acc: 0.9090\n",
      "Epoch 21/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1338 - acc: 0.9723Epoch 00021: val_acc did not improve\n",
      "200/200 [==============================] - 327s 2s/step - loss: 0.1338 - acc: 0.9724 - val_loss: 0.1699 - val_acc: 0.9563\n",
      "Epoch 22/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1393 - acc: 0.9720Epoch 00022: val_acc improved from 0.96133 to 0.96172, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-22-0.96-0.16.h5\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.1395 - acc: 0.9720 - val_loss: 0.1568 - val_acc: 0.9617\n",
      "Epoch 23/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1393 - acc: 0.9721Epoch 00023: val_acc did not improve\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.1394 - acc: 0.9721 - val_loss: 0.2245 - val_acc: 0.9418\n",
      "Epoch 24/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1343 - acc: 0.9736Epoch 00024: val_acc improved from 0.96172 to 0.96211, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-24-0.96-0.17.h5\n",
      "200/200 [==============================] - 332s 2s/step - loss: 0.1346 - acc: 0.9736 - val_loss: 0.1709 - val_acc: 0.9621\n",
      "Epoch 25/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1385 - acc: 0.9711Epoch 00025: val_acc improved from 0.96211 to 0.96328, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-25-0.96-0.16.h5\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.1386 - acc: 0.9711 - val_loss: 0.1591 - val_acc: 0.9633\n",
      "Epoch 26/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1358 - acc: 0.9730Epoch 00026: val_acc did not improve\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.1357 - acc: 0.9730 - val_loss: 0.1802 - val_acc: 0.9559\n",
      "Epoch 27/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1319 - acc: 0.9734Epoch 00027: val_acc did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 0.00020000000949949026.\n",
      "200/200 [==============================] - 327s 2s/step - loss: 0.1318 - acc: 0.9734 - val_loss: 0.1654 - val_acc: 0.9605\n",
      "Epoch 28/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1001 - acc: 0.9814Epoch 00028: val_acc improved from 0.96328 to 0.97891, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-28-0.98-0.09.h5\n",
      "200/200 [==============================] - 332s 2s/step - loss: 0.1001 - acc: 0.9814 - val_loss: 0.0949 - val_acc: 0.9789\n",
      "Epoch 29/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 1s - loss: 0.0961 - acc: 0.9821Epoch 00029: val_acc did not improve\n",
      "200/200 [==============================] - 329s 2s/step - loss: 0.0965 - acc: 0.9821 - val_loss: 0.1006 - val_acc: 0.9789\n",
      "Epoch 30/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1062 - acc: 0.9818Epoch 00030: val_acc improved from 0.97891 to 0.97930, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-30-0.98-0.10.h5\n",
      "200/200 [==============================] - 329s 2s/step - loss: 0.1057 - acc: 0.9819 - val_loss: 0.1032 - val_acc: 0.9793\n",
      "Epoch 31/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0963 - acc: 0.9834Epoch 00031: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0967 - acc: 0.9833 - val_loss: 0.1085 - val_acc: 0.9754\n",
      "Epoch 32/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0987 - acc: 0.9828Epoch 00032: val_acc improved from 0.97930 to 0.98008, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-32-0.98-0.10.h5\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.0986 - acc: 0.9828 - val_loss: 0.1013 - val_acc: 0.9801\n",
      "Epoch 33/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.1123 - acc: 0.9812Epoch 00033: val_acc improved from 0.98008 to 0.98164, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-33-0.98-0.09.h5\n",
      "200/200 [==============================] - 343s 2s/step - loss: 0.1122 - acc: 0.9812 - val_loss: 0.0909 - val_acc: 0.9816\n",
      "Epoch 34/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0937 - acc: 0.9838Epoch 00034: val_acc did not improve\n",
      "200/200 [==============================] - 335s 2s/step - loss: 0.0943 - acc: 0.9837 - val_loss: 0.1071 - val_acc: 0.9781\n",
      "Epoch 35/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0836 - acc: 0.9854Epoch 00035: val_acc did not improve\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.0835 - acc: 0.9854 - val_loss: 0.1056 - val_acc: 0.9801\n",
      "Epoch 36/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0939 - acc: 0.9835Epoch 00036: val_acc did not improve\n",
      "200/200 [==============================] - 330s 2s/step - loss: 0.0935 - acc: 0.9836 - val_loss: 0.0932 - val_acc: 0.9809\n",
      "Epoch 37/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0895 - acc: 0.9848Epoch 00037: val_acc did not improve\n",
      "200/200 [==============================] - 333s 2s/step - loss: 0.0896 - acc: 0.9848 - val_loss: 0.1159 - val_acc: 0.9797\n",
      "Epoch 38/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0915 - acc: 0.9841Epoch 00038: val_acc did not improve\n",
      "\n",
      "Epoch 00038: reducing learning rate to 4.0000001899898055e-05.\n",
      "200/200 [==============================] - 329s 2s/step - loss: 0.0918 - acc: 0.9841 - val_loss: 0.0942 - val_acc: 0.9816\n",
      "Epoch 39/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0888 - acc: 0.9853Epoch 00039: val_acc improved from 0.98164 to 0.98398, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-39-0.98-0.08.h5\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.0886 - acc: 0.9853 - val_loss: 0.0819 - val_acc: 0.9840\n",
      "Epoch 40/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0888 - acc: 0.9851Epoch 00040: val_acc did not improve\n",
      "200/200 [==============================] - 330s 2s/step - loss: 0.0892 - acc: 0.9851 - val_loss: 0.0863 - val_acc: 0.9824\n",
      "Epoch 41/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0947 - acc: 0.9849Epoch 00041: val_acc did not improve\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.0944 - acc: 0.9849 - val_loss: 0.1070 - val_acc: 0.9793\n",
      "Epoch 42/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0847 - acc: 0.9863Epoch 00042: val_acc did not improve\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.0844 - acc: 0.9863 - val_loss: 0.0951 - val_acc: 0.9824\n",
      "Epoch 43/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0854 - acc: 0.9862Epoch 00043: val_acc did not improve\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.0856 - acc: 0.9862 - val_loss: 0.0856 - val_acc: 0.9816\n",
      "Epoch 44/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0865 - acc: 0.9859Epoch 00044: val_acc improved from 0.98398 to 0.98516, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-44-0.99-0.08.h5\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.0866 - acc: 0.9859 - val_loss: 0.0810 - val_acc: 0.9852\n",
      "Epoch 45/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0849 - acc: 0.9863Epoch 00045: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0849 - acc: 0.9864 - val_loss: 0.0854 - val_acc: 0.9852\n",
      "Epoch 46/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0873 - acc: 0.9858Epoch 00046: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0873 - acc: 0.9858 - val_loss: 0.0997 - val_acc: 0.9777\n",
      "Epoch 47/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0847 - acc: 0.9866Epoch 00047: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0851 - acc: 0.9865 - val_loss: 0.0880 - val_acc: 0.9836\n",
      "Epoch 48/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0832 - acc: 0.9864Epoch 00048: val_acc did not improve\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0831 - acc: 0.9864 - val_loss: 0.1031 - val_acc: 0.9785\n",
      "Epoch 49/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0832 - acc: 0.9867Epoch 00049: val_acc did not improve\n",
      "\n",
      "Epoch 00049: reducing learning rate to 8.000000525498762e-06.\n",
      "200/200 [==============================] - 331s 2s/step - loss: 0.0832 - acc: 0.9866 - val_loss: 0.1016 - val_acc: 0.9805\n",
      "Epoch 50/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0815 - acc: 0.9872Epoch 00050: val_acc did not improve\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0816 - acc: 0.9871 - val_loss: 0.0769 - val_acc: 0.9844\n",
      "Epoch 51/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0746 - acc: 0.9885Epoch 00051: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0744 - acc: 0.9885 - val_loss: 0.1111 - val_acc: 0.9785\n",
      "Epoch 52/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0816 - acc: 0.9873Epoch 00052: val_acc did not improve\n",
      "200/200 [==============================] - 328s 2s/step - loss: 0.0820 - acc: 0.9873 - val_loss: 0.0941 - val_acc: 0.9828\n",
      "Epoch 53/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0803 - acc: 0.9875Epoch 00053: val_acc did not improve\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0805 - acc: 0.9875 - val_loss: 0.1141 - val_acc: 0.9797\n",
      "Epoch 54/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0784 - acc: 0.9878Epoch 00054: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0786 - acc: 0.9877 - val_loss: 0.1049 - val_acc: 0.9812\n",
      "Epoch 55/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0827 - acc: 0.9868Epoch 00055: val_acc did not improve\n",
      "\n",
      "Epoch 00055: reducing learning rate to 1.6000001778593287e-06.\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0823 - acc: 0.9869 - val_loss: 0.1183 - val_acc: 0.9781\n",
      "Epoch 56/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0829 - acc: 0.9866Epoch 00056: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0831 - acc: 0.9865 - val_loss: 0.0860 - val_acc: 0.9824\n",
      "Epoch 57/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0776 - acc: 0.9877Epoch 00057: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0781 - acc: 0.9877 - val_loss: 0.1097 - val_acc: 0.9785\n",
      "Epoch 58/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0833 - acc: 0.9867Epoch 00058: val_acc did not improve\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0832 - acc: 0.9867 - val_loss: 0.1039 - val_acc: 0.9805\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 1s - loss: 0.0772 - acc: 0.9881Epoch 00059: val_acc did not improve\n",
      "\n",
      "Epoch 00059: reducing learning rate to 3.200000264769187e-07.\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0769 - acc: 0.9881 - val_loss: 0.0789 - val_acc: 0.9828\n",
      "Epoch 60/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0785 - acc: 0.9885Epoch 00060: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0785 - acc: 0.9885 - val_loss: 0.0749 - val_acc: 0.9844\n",
      "Epoch 61/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0812 - acc: 0.9872Epoch 00061: val_acc did not improve\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0812 - acc: 0.9872 - val_loss: 0.0984 - val_acc: 0.9805\n",
      "Epoch 62/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0733 - acc: 0.9889Epoch 00062: val_acc did not improve\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0734 - acc: 0.9888 - val_loss: 0.1000 - val_acc: 0.9812\n",
      "Epoch 63/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0837 - acc: 0.9868Epoch 00063: val_acc did not improve\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0840 - acc: 0.9868 - val_loss: 0.0818 - val_acc: 0.9844\n",
      "Epoch 64/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0855 - acc: 0.9867Epoch 00064: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0858 - acc: 0.9866 - val_loss: 0.1029 - val_acc: 0.9816\n",
      "Epoch 65/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0800 - acc: 0.9874Epoch 00065: val_acc did not improve\n",
      "\n",
      "Epoch 00065: reducing learning rate to 6.400000529538374e-08.\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0803 - acc: 0.9873 - val_loss: 0.0944 - val_acc: 0.9801\n",
      "Epoch 66/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0832 - acc: 0.9869Epoch 00066: val_acc did not improve\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0831 - acc: 0.9869 - val_loss: 0.1127 - val_acc: 0.9754\n",
      "Epoch 67/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0842 - acc: 0.9866Epoch 00067: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0842 - acc: 0.9866 - val_loss: 0.1018 - val_acc: 0.9812\n",
      "Epoch 68/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0793 - acc: 0.9870Epoch 00068: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0790 - acc: 0.9870 - val_loss: 0.1170 - val_acc: 0.9801\n",
      "Epoch 69/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0817 - acc: 0.9872Epoch 00069: val_acc did not improve\n",
      "\n",
      "Epoch 00069: reducing learning rate to 1.2800001059076749e-08.\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0817 - acc: 0.9872 - val_loss: 0.0944 - val_acc: 0.9824\n",
      "Epoch 70/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0763 - acc: 0.9879Epoch 00070: val_acc did not improve\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0768 - acc: 0.9879 - val_loss: 0.1005 - val_acc: 0.9812\n",
      "Epoch 71/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0850 - acc: 0.9861Epoch 00071: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0847 - acc: 0.9862 - val_loss: 0.1084 - val_acc: 0.9770\n",
      "Epoch 72/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0816 - acc: 0.9867Epoch 00072: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0813 - acc: 0.9868 - val_loss: 0.1256 - val_acc: 0.9762\n",
      "Epoch 73/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0777 - acc: 0.9878Epoch 00073: val_acc did not improve\n",
      "\n",
      "Epoch 00073: reducing learning rate to 1e-08.\n",
      "200/200 [==============================] - 325s 2s/step - loss: 0.0777 - acc: 0.9879 - val_loss: 0.0895 - val_acc: 0.9805\n",
      "Epoch 74/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0809 - acc: 0.9876Epoch 00074: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0814 - acc: 0.9875 - val_loss: 0.1009 - val_acc: 0.9805\n",
      "Epoch 75/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0838 - acc: 0.9872Epoch 00075: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0837 - acc: 0.9872 - val_loss: 0.1049 - val_acc: 0.9812\n",
      "Epoch 76/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0767 - acc: 0.9874Epoch 00076: val_acc improved from 0.98516 to 0.98594, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-76-0.99-0.07.h5\n",
      "200/200 [==============================] - 326s 2s/step - loss: 0.0768 - acc: 0.9874 - val_loss: 0.0656 - val_acc: 0.9859\n",
      "Epoch 77/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0750 - acc: 0.9883Epoch 00077: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0751 - acc: 0.9883 - val_loss: 0.0947 - val_acc: 0.9812\n",
      "Epoch 78/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0799 - acc: 0.9873Epoch 00078: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0796 - acc: 0.9873 - val_loss: 0.0887 - val_acc: 0.9832\n",
      "Epoch 79/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0791 - acc: 0.9883Epoch 00079: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0791 - acc: 0.9883 - val_loss: 0.0884 - val_acc: 0.9816\n",
      "Epoch 80/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0770 - acc: 0.9881Epoch 00080: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0771 - acc: 0.9881 - val_loss: 0.1006 - val_acc: 0.9785\n",
      "Epoch 81/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0862 - acc: 0.9868Epoch 00081: val_acc did not improve\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0865 - acc: 0.9868 - val_loss: 0.1061 - val_acc: 0.9766\n",
      "Epoch 82/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0830 - acc: 0.9870Epoch 00082: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0829 - acc: 0.9870 - val_loss: 0.0861 - val_acc: 0.9832\n",
      "Epoch 83/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0756 - acc: 0.9881Epoch 00083: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0755 - acc: 0.9881 - val_loss: 0.1130 - val_acc: 0.9801\n",
      "Epoch 84/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0808 - acc: 0.9874Epoch 00084: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0808 - acc: 0.9874 - val_loss: 0.1036 - val_acc: 0.9801\n",
      "Epoch 85/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0745 - acc: 0.9885Epoch 00085: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0747 - acc: 0.9884 - val_loss: 0.0907 - val_acc: 0.9812\n",
      "Epoch 86/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0832 - acc: 0.9868Epoch 00086: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0830 - acc: 0.9868 - val_loss: 0.0847 - val_acc: 0.9828\n",
      "Epoch 87/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0797 - acc: 0.9876Epoch 00087: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0797 - acc: 0.9876 - val_loss: 0.0914 - val_acc: 0.9816\n",
      "Epoch 88/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0795 - acc: 0.9872Epoch 00088: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0798 - acc: 0.9871 - val_loss: 0.0958 - val_acc: 0.9789\n",
      "Epoch 89/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0834 - acc: 0.9870Epoch 00089: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0835 - acc: 0.9869 - val_loss: 0.0904 - val_acc: 0.9824\n",
      "Epoch 90/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 1s - loss: 0.0867 - acc: 0.9868Epoch 00090: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0865 - acc: 0.9868 - val_loss: 0.0969 - val_acc: 0.9809\n",
      "Epoch 91/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0775 - acc: 0.9882Epoch 00091: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0777 - acc: 0.9882 - val_loss: 0.0923 - val_acc: 0.9820\n",
      "Epoch 92/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0801 - acc: 0.9873Epoch 00092: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0802 - acc: 0.9873 - val_loss: 0.1117 - val_acc: 0.9781\n",
      "Epoch 93/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0784 - acc: 0.9876Epoch 00093: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0785 - acc: 0.9876 - val_loss: 0.0836 - val_acc: 0.9832\n",
      "Epoch 94/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0799 - acc: 0.9876Epoch 00094: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0797 - acc: 0.9877 - val_loss: 0.1017 - val_acc: 0.9777\n",
      "Epoch 95/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0793 - acc: 0.9875Epoch 00095: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0795 - acc: 0.9875 - val_loss: 0.1087 - val_acc: 0.9793\n",
      "Epoch 96/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0730 - acc: 0.9881Epoch 00096: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0731 - acc: 0.9880 - val_loss: 0.0972 - val_acc: 0.9797\n",
      "Epoch 97/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0817 - acc: 0.9874Epoch 00097: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0816 - acc: 0.9875 - val_loss: 0.0920 - val_acc: 0.9824\n",
      "Epoch 98/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0759 - acc: 0.9879Epoch 00098: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0756 - acc: 0.9880 - val_loss: 0.0944 - val_acc: 0.9816\n",
      "Epoch 99/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0831 - acc: 0.9872Epoch 00099: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0831 - acc: 0.9872 - val_loss: 0.1091 - val_acc: 0.9789\n",
      "Epoch 100/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0827 - acc: 0.9868Epoch 00100: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0824 - acc: 0.9868 - val_loss: 0.1136 - val_acc: 0.9801\n",
      "Epoch 101/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0727 - acc: 0.9883Epoch 00101: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0727 - acc: 0.9883 - val_loss: 0.0865 - val_acc: 0.9836\n",
      "Epoch 102/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0839 - acc: 0.9868Epoch 00102: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0842 - acc: 0.9868 - val_loss: 0.1144 - val_acc: 0.9770\n",
      "Epoch 103/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0775 - acc: 0.9880Epoch 00103: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0775 - acc: 0.9880 - val_loss: 0.1012 - val_acc: 0.9797\n",
      "Epoch 104/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0804 - acc: 0.9876Epoch 00104: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0804 - acc: 0.9876 - val_loss: 0.0981 - val_acc: 0.9812\n",
      "Epoch 105/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0814 - acc: 0.9871Epoch 00105: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0818 - acc: 0.9870 - val_loss: 0.0988 - val_acc: 0.9832\n",
      "Epoch 106/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0777 - acc: 0.9872Epoch 00106: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0774 - acc: 0.9872 - val_loss: 0.1079 - val_acc: 0.9805\n",
      "Epoch 107/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0778 - acc: 0.9879Epoch 00107: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0779 - acc: 0.9879 - val_loss: 0.0804 - val_acc: 0.9836\n",
      "Epoch 108/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0797 - acc: 0.9879Epoch 00108: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0797 - acc: 0.9879 - val_loss: 0.0957 - val_acc: 0.9805\n",
      "Epoch 109/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0817 - acc: 0.9876Epoch 00109: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0816 - acc: 0.9876 - val_loss: 0.1090 - val_acc: 0.9777\n",
      "Epoch 110/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0821 - acc: 0.9874Epoch 00110: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0821 - acc: 0.9874 - val_loss: 0.0986 - val_acc: 0.9805\n",
      "Epoch 111/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0803 - acc: 0.9874Epoch 00111: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0803 - acc: 0.9873 - val_loss: 0.0843 - val_acc: 0.9836\n",
      "Epoch 112/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0822 - acc: 0.9872Epoch 00112: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0822 - acc: 0.9871 - val_loss: 0.1126 - val_acc: 0.9773\n",
      "Epoch 113/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0811 - acc: 0.9870Epoch 00113: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0814 - acc: 0.9869 - val_loss: 0.1072 - val_acc: 0.9824\n",
      "Epoch 114/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0790 - acc: 0.9881Epoch 00114: val_acc improved from 0.98594 to 0.98633, saving model to model-pseudo-88-melspec-inception-resnet-v2-0-114-0.99-0.07.h5\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0791 - acc: 0.9880 - val_loss: 0.0680 - val_acc: 0.9863\n",
      "Epoch 115/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0806 - acc: 0.9872Epoch 00115: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0806 - acc: 0.9872 - val_loss: 0.0953 - val_acc: 0.9828\n",
      "Epoch 116/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0825 - acc: 0.9873Epoch 00116: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0825 - acc: 0.9873 - val_loss: 0.1063 - val_acc: 0.9785\n",
      "Epoch 117/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0833 - acc: 0.9865Epoch 00117: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0837 - acc: 0.9864 - val_loss: 0.1009 - val_acc: 0.9801\n",
      "Epoch 118/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0792 - acc: 0.9877Epoch 00118: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0793 - acc: 0.9877 - val_loss: 0.0874 - val_acc: 0.9844\n",
      "Epoch 119/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0775 - acc: 0.9884Epoch 00119: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0777 - acc: 0.9884 - val_loss: 0.0979 - val_acc: 0.9789\n",
      "Epoch 120/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0786 - acc: 0.9869Epoch 00120: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0785 - acc: 0.9870 - val_loss: 0.0932 - val_acc: 0.9820\n",
      "Epoch 121/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0889 - acc: 0.9861Epoch 00121: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0894 - acc: 0.9861 - val_loss: 0.1004 - val_acc: 0.9801\n",
      "Epoch 122/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 1s - loss: 0.0817 - acc: 0.9881Epoch 00122: val_acc did not improve\n",
      "200/200 [==============================] - 318s 2s/step - loss: 0.0818 - acc: 0.9880 - val_loss: 0.1116 - val_acc: 0.9797\n",
      "Epoch 123/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0781 - acc: 0.9876Epoch 00123: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0781 - acc: 0.9876 - val_loss: 0.0810 - val_acc: 0.9836\n",
      "Epoch 124/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0793 - acc: 0.9871Epoch 00124: val_acc did not improve\n",
      "200/200 [==============================] - 324s 2s/step - loss: 0.0793 - acc: 0.9871 - val_loss: 0.1085 - val_acc: 0.9797\n",
      "Epoch 125/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0833 - acc: 0.9870Epoch 00125: val_acc did not improve\n",
      "200/200 [==============================] - 318s 2s/step - loss: 0.0835 - acc: 0.9870 - val_loss: 0.0890 - val_acc: 0.9832\n",
      "Epoch 126/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0772 - acc: 0.9877Epoch 00126: val_acc did not improve\n",
      "200/200 [==============================] - 318s 2s/step - loss: 0.0769 - acc: 0.9878 - val_loss: 0.0993 - val_acc: 0.9801\n",
      "Epoch 127/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0766 - acc: 0.9878Epoch 00127: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0769 - acc: 0.9878 - val_loss: 0.0926 - val_acc: 0.9828\n",
      "Epoch 128/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0828 - acc: 0.9876Epoch 00128: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0826 - acc: 0.9875 - val_loss: 0.1068 - val_acc: 0.9793\n",
      "Epoch 129/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0785 - acc: 0.9881Epoch 00129: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0781 - acc: 0.9881 - val_loss: 0.1112 - val_acc: 0.9797\n",
      "Epoch 130/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0780 - acc: 0.9878Epoch 00130: val_acc did not improve\n",
      "200/200 [==============================] - 323s 2s/step - loss: 0.0780 - acc: 0.9878 - val_loss: 0.1024 - val_acc: 0.9812\n",
      "Epoch 131/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0756 - acc: 0.9884Epoch 00131: val_acc did not improve\n",
      "200/200 [==============================] - 317s 2s/step - loss: 0.0757 - acc: 0.9884 - val_loss: 0.1278 - val_acc: 0.9754\n",
      "Epoch 132/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0757 - acc: 0.9885Epoch 00132: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0757 - acc: 0.9886 - val_loss: 0.0906 - val_acc: 0.9824\n",
      "Epoch 133/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0768 - acc: 0.9878Epoch 00133: val_acc did not improve\n",
      "200/200 [==============================] - 318s 2s/step - loss: 0.0768 - acc: 0.9879 - val_loss: 0.1025 - val_acc: 0.9793\n",
      "Epoch 134/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0815 - acc: 0.9875Epoch 00134: val_acc did not improve\n",
      "200/200 [==============================] - 321s 2s/step - loss: 0.0814 - acc: 0.9875 - val_loss: 0.0804 - val_acc: 0.9844\n",
      "Epoch 135/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0744 - acc: 0.9882Epoch 00135: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0741 - acc: 0.9883 - val_loss: 0.0902 - val_acc: 0.9809\n",
      "Epoch 136/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0849 - acc: 0.9869Epoch 00136: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0848 - acc: 0.9869 - val_loss: 0.0846 - val_acc: 0.9832\n",
      "Epoch 137/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0827 - acc: 0.9868Epoch 00137: val_acc did not improve\n",
      "200/200 [==============================] - 318s 2s/step - loss: 0.0827 - acc: 0.9868 - val_loss: 0.1019 - val_acc: 0.9789\n",
      "Epoch 138/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0812 - acc: 0.9869Epoch 00138: val_acc did not improve\n",
      "200/200 [==============================] - 322s 2s/step - loss: 0.0812 - acc: 0.9869 - val_loss: 0.0756 - val_acc: 0.9836\n",
      "Epoch 139/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0823 - acc: 0.9872Epoch 00140: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0822 - acc: 0.9872 - val_loss: 0.0923 - val_acc: 0.9812\n",
      "Epoch 141/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0778 - acc: 0.9879Epoch 00141: val_acc did not improve\n",
      "200/200 [==============================] - 318s 2s/step - loss: 0.0778 - acc: 0.9880 - val_loss: 0.0892 - val_acc: 0.9805\n",
      "Epoch 142/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0855 - acc: 0.9865Epoch 00142: val_acc did not improve\n",
      "200/200 [==============================] - 315s 2s/step - loss: 0.0855 - acc: 0.9866 - val_loss: 0.1121 - val_acc: 0.9785\n",
      "Epoch 143/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0846 - acc: 0.9869Epoch 00143: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0845 - acc: 0.9870 - val_loss: 0.0794 - val_acc: 0.9824\n",
      "Epoch 144/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0830 - acc: 0.9866Epoch 00144: val_acc did not improve\n",
      "200/200 [==============================] - 317s 2s/step - loss: 0.0831 - acc: 0.9866 - val_loss: 0.0854 - val_acc: 0.9824\n",
      "Epoch 145/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0793 - acc: 0.9871Epoch 00145: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0793 - acc: 0.9871 - val_loss: 0.0725 - val_acc: 0.9852\n",
      "Epoch 146/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0769 - acc: 0.9874Epoch 00146: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0774 - acc: 0.9873 - val_loss: 0.1030 - val_acc: 0.9797\n",
      "Epoch 147/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0777 - acc: 0.9880Epoch 00147: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0775 - acc: 0.9880 - val_loss: 0.0957 - val_acc: 0.9797\n",
      "Epoch 148/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0840 - acc: 0.9864Epoch 00148: val_acc did not improve\n",
      "200/200 [==============================] - 317s 2s/step - loss: 0.0841 - acc: 0.9864 - val_loss: 0.1024 - val_acc: 0.9797\n",
      "Epoch 149/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0875 - acc: 0.9863Epoch 00149: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0874 - acc: 0.9863 - val_loss: 0.0964 - val_acc: 0.9836\n",
      "Epoch 150/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0756 - acc: 0.9887Epoch 00150: val_acc did not improve\n",
      "200/200 [==============================] - 320s 2s/step - loss: 0.0753 - acc: 0.9888 - val_loss: 0.1038 - val_acc: 0.9824\n",
      "Epoch 151/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0747 - acc: 0.9879Epoch 00151: val_acc did not improve\n",
      "200/200 [==============================] - 319s 2s/step - loss: 0.0745 - acc: 0.9879 - val_loss: 0.0856 - val_acc: 0.9820\n",
      "Epoch 152/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0794 - acc: 0.9878"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2279\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2280\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2281\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-43cf86b8b799>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2167\u001b[0m                                 \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m                                 \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2169\u001b[0;31m                                 use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   2170\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m                             \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2312\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0menqueuer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    696\u001b[0m                     \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                     \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_manager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "model = keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=num_classes)\n",
    "#model = keras.applications.xception.Xception(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=num_classes)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "#model = load_model('model-xception-0-19-0.98-0.08.h5')\n",
    "checkpoint = ModelCheckpoint('model-pseudo-88-melspec-inception-resnet-v2-0-{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.h5',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='max')\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-8, verbose=1)\n",
    "callback_list = [checkpoint, reduce_lr]\n",
    "\n",
    "train_generator = train_data_generator()\n",
    "validation_generator = validation_data_generator()\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=200, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callback_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=20,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_taken())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
