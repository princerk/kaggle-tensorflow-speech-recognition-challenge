{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np # linear algebra\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, Convolution2D, Activation\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import os\n",
    "import random\n",
    "from scipy.io import wavfile\n",
    "from subprocess import check_output\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import acoustics\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import random\n",
    "\n",
    "def timer():\n",
    "    time_start = [int(time.time())]\n",
    "    time_start_overall = [int(time.time())]\n",
    "\n",
    "    def time_taken():\n",
    "        time_now = int(time.time())\n",
    "        om, os = divmod(time_now - time_start_overall[0], 60)\n",
    "        m, s = divmod(time_now - time_start[0], 60)\n",
    "        time_start[0] = time_now\n",
    "        return ' overall_time: ' + str(om) + 'm' + str(os) + 's' + ' time_from_previous_call: ' + str(m) + 'm' + str(s) + 's'\n",
    "    return time_taken\n",
    "time_taken = timer()\n",
    "\n",
    "sr = 16000\n",
    "\n",
    "unknowns = 'bed bird cat dog eight five four happy house marvin nine one seven sheila six three tree two wow zero'.split()\n",
    "knowns = 'yes no up down left right on off stop go'.split()\n",
    "silence = 'silence'.split()\n",
    "labels = knowns + silence + ['unknown'] \n",
    "num_classes = len(labels)\n",
    "\n",
    "LABEL_TO_FILE_NAMES = {}\n",
    "VALIDATION_LABEL_TO_FILE_NAMES = {}\n",
    "TRAIN_LABEL_TO_FILE_NAMES = {}\n",
    "FILE_TO_LABEL = {}\n",
    "with open('train.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        TRAIN_LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        TRAIN_LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        FILE_TO_LABEL[line[0]] = line[1]\n",
    "with open('validation.csv') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(',')\n",
    "        LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        VALIDATION_LABEL_TO_FILE_NAMES.setdefault(line[1], [])\n",
    "        VALIDATION_LABEL_TO_FILE_NAMES[line[1]].append(line[0])\n",
    "        FILE_TO_LABEL[line[0]] = line[1]\n",
    "\n",
    "\n",
    "def file_to_sample(filename):\n",
    "    samples, _ = librosa.load(filename, sr=sr)\n",
    "    return samples\n",
    "    \n",
    "\n",
    "UNCOLORED_NOISES = []\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/doing_the_dishes.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/dude_miaowing.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/exercise_bike.wav', sr=sr)[0].tolist()\n",
    "UNCOLORED_NOISES += librosa.load('../train/audio/_background_noise_/running_tap.wav', sr=sr)[0].tolist()\n",
    "\n",
    "\n",
    "def get_silence():\n",
    "    choice = np.random.choice([0, 1, 2, 4], p=[0.01, 0.10, 0.70, 0.19])\n",
    "    if choice == 0:\n",
    "        return np.zeros((16000))\n",
    "    elif choice == 1:\n",
    "        idx = random.randint(0, len(UNCOLORED_NOISES) - sr)\n",
    "        return np.array(UNCOLORED_NOISES[idx:idx+sr], dtype=np.float32)\n",
    "    elif choice == 3:\n",
    "        return np.array(acoustics.generator.noise(16000, color=np.random.choice(['pink', 'white']))/3, np.float32)\n",
    "    else:\n",
    "        random_silence_file = np.random.choice(LABEL_TO_FILE_NAMES['silence'])\n",
    "        return file_to_sample(random_silence_file)\n",
    "\n",
    "def pad_zeros(samples):\n",
    "    if len(samples) < sr:\n",
    "        diff = sr - len(samples)\n",
    "        diff_div = diff // 2\n",
    "        samples = np.lib.pad(samples, (diff_div, diff - diff_div), 'constant', constant_values = (0, 0))\n",
    "    return samples\n",
    "\n",
    "def pitch_shift(samples, sr=sr):\n",
    "    return librosa.effects.pitch_shift(samples, sr=sr, n_steps=random.randint(1, 5))\n",
    "\n",
    "def get_shuffled_XY(X, Y):\n",
    "    m = X.shape[0]\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "    return shuffled_X, shuffled_Y\n",
    "\n",
    "def time_shift(arr):\n",
    "    num = np.random.uniform(0, 0.2) * len(arr)\n",
    "    num = int(num)\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = 0\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = 0\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result = arr\n",
    "    return np.array(result)\n",
    "        \n",
    "def flip_transform(wave):\n",
    "    if np.random.choice([0, 1]):\n",
    "        return -wave\n",
    "\n",
    "def noise_mix(wave):\n",
    "    noise = get_silence()\n",
    "    noise_limit = random.uniform(0, 0.1)\n",
    "    wave = (1 - noise_limit) * wave + noise_limit * noise\n",
    "    return wave\n",
    "\n",
    "def get_melspectrogram(samples):\n",
    "    S = librosa.feature.melspectrogram(samples, sr=sr, n_mels=72, hop_length=223, n_fft=512)\n",
    "    spec = librosa.power_to_db(S, ref=np.max)\n",
    "    spec = np.repeat(spec[np.newaxis,:,:,np.newaxis], 3, axis=3)\n",
    "    return spec\n",
    "\n",
    "def get_transformed_samples(samples):\n",
    "    samples = pad_zeros(samples)\n",
    "    samples = time_shift(samples)\n",
    "    samples = noise_mix(samples)\n",
    "    if np.random.choice([0, 1]):\n",
    "        samples = -samples\n",
    "    stdx = np.std(samples)\n",
    "    if stdx:\n",
    "        sampels = samples / stdx\n",
    "    #samples = flip_transform(samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'silence', 'unknown']\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing train label to filename lengths\n",
      "right 7284\n",
      "down 6203\n",
      "up 6300\n",
      "no 7539\n",
      "go 5464\n",
      "left 6668\n",
      "silence 10208\n",
      "off 6810\n",
      "stop 7044\n",
      "on 7637\n",
      "unknown 88628\n",
      "yes 6562\n",
      "printing validation label to filename lengths\n",
      "right 809\n",
      "down 689\n",
      "up 700\n",
      "no 837\n",
      "go 607\n",
      "left 740\n",
      "silence 1134\n",
      "off 756\n",
      "stop 782\n",
      "on 848\n",
      "unknown 9847\n",
      "yes 729\n"
     ]
    }
   ],
   "source": [
    "print('printing train label to filename lengths')\n",
    "for label, filenames in TRAIN_LABEL_TO_FILE_NAMES.items():\n",
    "    print(label, len(filenames))\n",
    "print('printing validation label to filename lengths')\n",
    "for label, filenames in VALIDATION_LABEL_TO_FILE_NAMES.items():\n",
    "    print(label, len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " overall_time: 0m0s time_from_previous_call: 0m0s\n"
     ]
    }
   ],
   "source": [
    "print(time_taken())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "def validation_data_generator():\n",
    "    XV = np.zeros((batch_size, 72, 72, 3))\n",
    "    YV = np.zeros((batch_size, num_classes))\n",
    "    while True:\n",
    "        for idx in range(batch_size):\n",
    "            random_label = np.random.choice(labels)\n",
    "            if random_label == 'silence':\n",
    "                samples = get_silence()\n",
    "            else:\n",
    "                random_filename = np.random.choice(VALIDATION_LABEL_TO_FILE_NAMES[random_label])\n",
    "                samples = file_to_sample(random_filename)\n",
    "                samples = get_transformed_samples(samples)\n",
    "            spec = get_melspectrogram(samples)\n",
    "            XV[idx, :, :] = spec\n",
    "            this_Y = [labels.index(random_label)]\n",
    "            this_Y = keras.utils.to_categorical(np.array(this_Y).astype(np.float32), num_classes)\n",
    "            YV[idx, :] = this_Y\n",
    "        yield XV, YV\n",
    "\n",
    "def train_data_generator():\n",
    "    XT = np.zeros((batch_size, 72, 72, 3))\n",
    "    YT = np.zeros((batch_size, num_classes))\n",
    "    while True:\n",
    "        for idx in range(batch_size):\n",
    "            random_label = np.random.choice(labels)\n",
    "            if random_label == 'silence':\n",
    "                samples = get_silence()\n",
    "            else:\n",
    "                random_filename = np.random.choice(TRAIN_LABEL_TO_FILE_NAMES[random_label])\n",
    "                samples = file_to_sample(random_filename)\n",
    "                samples = get_transformed_samples(samples)\n",
    "            spec = get_melspectrogram(samples)\n",
    "            XT[idx, :, :] = spec\n",
    "            this_Y = [labels.index(random_label)]\n",
    "            this_Y = keras.utils.to_categorical(np.array(this_Y).astype(np.float32), num_classes)\n",
    "            YT[idx, :] = this_Y\n",
    "        yield XT, YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0698 - acc: 0.9801Epoch 00001: val_acc improved from -inf to 0.96484, saving model to model-xception-1-01-0.96-0.12.h5\n",
      "200/200 [==============================] - 288s 1s/step - loss: 0.0701 - acc: 0.9800 - val_loss: 0.1182 - val_acc: 0.9648\n",
      "Epoch 2/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0671 - acc: 0.9796Epoch 00002: val_acc improved from 0.96484 to 0.97148, saving model to model-xception-1-02-0.97-0.09.h5\n",
      "200/200 [==============================] - 250s 1s/step - loss: 0.0669 - acc: 0.9797 - val_loss: 0.0900 - val_acc: 0.9715\n",
      "Epoch 3/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0726 - acc: 0.9795Epoch 00003: val_acc did not improve\n",
      "200/200 [==============================] - 244s 1s/step - loss: 0.0723 - acc: 0.9796 - val_loss: 0.1207 - val_acc: 0.9672\n",
      "Epoch 4/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0672 - acc: 0.9813Epoch 00004: val_acc did not improve\n",
      "200/200 [==============================] - 239s 1s/step - loss: 0.0672 - acc: 0.9813 - val_loss: 0.1020 - val_acc: 0.9691\n",
      "Epoch 5/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0672 - acc: 0.9801Epoch 00005: val_acc did not improve\n",
      "200/200 [==============================] - 238s 1s/step - loss: 0.0673 - acc: 0.9801 - val_loss: 0.1076 - val_acc: 0.9688\n",
      "Epoch 6/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0700 - acc: 0.9811Epoch 00006: val_acc did not improve\n",
      "200/200 [==============================] - 232s 1s/step - loss: 0.0698 - acc: 0.9812 - val_loss: 0.1169 - val_acc: 0.9695\n",
      "Epoch 7/200\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0675 - acc: 0.9813Epoch 00007: val_acc improved from 0.97148 to 0.97617, saving model to model-xception-1-07-0.98-0.08.h5\n",
      "200/200 [==============================] - 233s 1s/step - loss: 0.0675 - acc: 0.9813 - val_loss: 0.0779 - val_acc: 0.9762\n",
      "Epoch 8/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0692 - acc: 0.9806Epoch 00008: val_acc did not improve\n",
      "200/200 [==============================] - 233s 1s/step - loss: 0.0689 - acc: 0.9807 - val_loss: 0.0919 - val_acc: 0.9723\n",
      "Epoch 9/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0627 - acc: 0.9823Epoch 00009: val_acc did not improve\n",
      "200/200 [==============================] - 230s 1s/step - loss: 0.0626 - acc: 0.9823 - val_loss: 0.0972 - val_acc: 0.9734\n",
      "Epoch 10/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9821Epoch 00010: val_acc did not improve\n",
      "200/200 [==============================] - 229s 1s/step - loss: 0.0676 - acc: 0.9821 - val_loss: 0.1238 - val_acc: 0.9684\n",
      "Epoch 11/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0677 - acc: 0.9810Epoch 00011: val_acc did not improve\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.0676 - acc: 0.9810 - val_loss: 0.0856 - val_acc: 0.9742\n",
      "Epoch 12/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9832Epoch 00012: val_acc did not improve\n",
      "\n",
      "Epoch 00012: reducing learning rate to 4.0000001899898055e-05.\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0646 - acc: 0.9833 - val_loss: 0.0956 - val_acc: 0.9723\n",
      "Epoch 13/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9838Epoch 00013: val_acc did not improve\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0616 - acc: 0.9838 - val_loss: 0.0989 - val_acc: 0.9734\n",
      "Epoch 14/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9865Epoch 00014: val_acc did not improve\n",
      "200/200 [==============================] - 228s 1s/step - loss: 0.0500 - acc: 0.9866 - val_loss: 0.0897 - val_acc: 0.9750\n",
      "Epoch 15/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9858Epoch 00015: val_acc improved from 0.97617 to 0.97656, saving model to model-xception-1-15-0.98-0.08.h5\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0517 - acc: 0.9858 - val_loss: 0.0780 - val_acc: 0.9766\n",
      "Epoch 16/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9879Epoch 00016: val_acc did not improve\n",
      "\n",
      "Epoch 00016: reducing learning rate to 8.000000525498762e-06.\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.0472 - acc: 0.9880 - val_loss: 0.0841 - val_acc: 0.9766\n",
      "Epoch 17/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9866Epoch 00017: val_acc did not improve\n",
      "200/200 [==============================] - 228s 1s/step - loss: 0.0480 - acc: 0.9866 - val_loss: 0.1127 - val_acc: 0.9734\n",
      "Epoch 18/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9878Epoch 00018: val_acc did not improve\n",
      "200/200 [==============================] - 228s 1s/step - loss: 0.0501 - acc: 0.9878 - val_loss: 0.0963 - val_acc: 0.9750\n",
      "Epoch 19/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9882Epoch 00019: val_acc did not improve\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0454 - acc: 0.9882 - val_loss: 0.1055 - val_acc: 0.9723\n",
      "Epoch 20/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9878Epoch 00020: val_acc did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 1.6000001778593287e-06.\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0481 - acc: 0.9877 - val_loss: 0.0963 - val_acc: 0.9723\n",
      "Epoch 21/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9876Epoch 00021: val_acc improved from 0.97656 to 0.98203, saving model to model-xception-1-21-0.98-0.06.h5\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.0481 - acc: 0.9877 - val_loss: 0.0619 - val_acc: 0.9820\n",
      "Epoch 22/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9878Epoch 00022: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0448 - acc: 0.9877 - val_loss: 0.0900 - val_acc: 0.9793\n",
      "Epoch 23/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9883Epoch 00023: val_acc did not improve\n",
      "200/200 [==============================] - 222s 1s/step - loss: 0.0420 - acc: 0.9884 - val_loss: 0.0838 - val_acc: 0.9738\n",
      "Epoch 24/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9879Epoch 00024: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0447 - acc: 0.9880 - val_loss: 0.0778 - val_acc: 0.9785\n",
      "Epoch 25/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9876Epoch 00025: val_acc did not improve\n",
      "200/200 [==============================] - 223s 1s/step - loss: 0.0470 - acc: 0.9876 - val_loss: 0.0798 - val_acc: 0.9789\n",
      "Epoch 26/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9878Epoch 00026: val_acc did not improve\n",
      "\n",
      "Epoch 00026: reducing learning rate to 3.200000264769187e-07.\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0450 - acc: 0.9878 - val_loss: 0.0844 - val_acc: 0.9750\n",
      "Epoch 27/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9878Epoch 00027: val_acc did not improve\n",
      "200/200 [==============================] - 228s 1s/step - loss: 0.0456 - acc: 0.9877 - val_loss: 0.0995 - val_acc: 0.9750\n",
      "Epoch 28/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9879Epoch 00028: val_acc did not improve\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0432 - acc: 0.9880 - val_loss: 0.0977 - val_acc: 0.9742\n",
      "Epoch 29/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9877Epoch 00029: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0441 - acc: 0.9877 - val_loss: 0.0846 - val_acc: 0.9758\n",
      "Epoch 30/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9879Epoch 00030: val_acc did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 6.400000529538374e-08.\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.0452 - acc: 0.9879 - val_loss: 0.0876 - val_acc: 0.9789\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9884Epoch 00031: val_acc did not improve\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0453 - acc: 0.9884 - val_loss: 0.0972 - val_acc: 0.9730\n",
      "Epoch 32/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9889Epoch 00032: val_acc did not improve\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0427 - acc: 0.9888 - val_loss: 0.0782 - val_acc: 0.9789\n",
      "Epoch 33/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9874Epoch 00033: val_acc did not improve\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.0474 - acc: 0.9875 - val_loss: 0.1033 - val_acc: 0.9719\n",
      "Epoch 34/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9878Epoch 00034: val_acc did not improve\n",
      "\n",
      "Epoch 00034: reducing learning rate to 1.2800001059076749e-08.\n",
      "200/200 [==============================] - 222s 1s/step - loss: 0.0464 - acc: 0.9878 - val_loss: 0.0742 - val_acc: 0.9777\n",
      "Epoch 35/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9877Epoch 00035: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0471 - acc: 0.9876 - val_loss: 0.0981 - val_acc: 0.9727\n",
      "Epoch 36/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9870Epoch 00036: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0490 - acc: 0.9870 - val_loss: 0.0754 - val_acc: 0.9781\n",
      "Epoch 37/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9878Epoch 00037: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0451 - acc: 0.9879 - val_loss: 0.0774 - val_acc: 0.9781\n",
      "Epoch 38/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9857Epoch 00038: val_acc did not improve\n",
      "\n",
      "Epoch 00038: reducing learning rate to 1e-08.\n",
      "200/200 [==============================] - 227s 1s/step - loss: 0.0504 - acc: 0.9857 - val_loss: 0.0786 - val_acc: 0.9762\n",
      "Epoch 39/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9884Epoch 00039: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0411 - acc: 0.9884 - val_loss: 0.0859 - val_acc: 0.9797\n",
      "Epoch 40/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9872Epoch 00040: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0449 - acc: 0.9873 - val_loss: 0.0750 - val_acc: 0.9789\n",
      "Epoch 41/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9873Epoch 00041: val_acc did not improve\n",
      "200/200 [==============================] - 224s 1s/step - loss: 0.0467 - acc: 0.9872 - val_loss: 0.0760 - val_acc: 0.9766\n",
      "Epoch 42/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9878Epoch 00042: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0446 - acc: 0.9878 - val_loss: 0.0939 - val_acc: 0.9727\n",
      "Epoch 43/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9894Epoch 00043: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0417 - acc: 0.9894 - val_loss: 0.0694 - val_acc: 0.9797\n",
      "Epoch 44/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9882Epoch 00044: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0471 - acc: 0.9881 - val_loss: 0.1035 - val_acc: 0.9734\n",
      "Epoch 45/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9869Epoch 00045: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0524 - acc: 0.9868 - val_loss: 0.0706 - val_acc: 0.9801\n",
      "Epoch 46/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9885Epoch 00046: val_acc did not improve\n",
      "200/200 [==============================] - 223s 1s/step - loss: 0.0415 - acc: 0.9886 - val_loss: 0.0773 - val_acc: 0.9781\n",
      "Epoch 47/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9890Epoch 00047: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0411 - acc: 0.9891 - val_loss: 0.0960 - val_acc: 0.9758\n",
      "Epoch 48/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9868Epoch 00048: val_acc did not improve\n",
      "200/200 [==============================] - 226s 1s/step - loss: 0.0507 - acc: 0.9868 - val_loss: 0.0901 - val_acc: 0.9734\n",
      "Epoch 49/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9887Epoch 00049: val_acc did not improve\n",
      "200/200 [==============================] - 224s 1s/step - loss: 0.0413 - acc: 0.9886 - val_loss: 0.0680 - val_acc: 0.9820\n",
      "Epoch 50/200\n",
      "199/200 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9873Epoch 00050: val_acc did not improve\n",
      "200/200 [==============================] - 225s 1s/step - loss: 0.0481 - acc: 0.9872 - val_loss: 0.0706 - val_acc: 0.9816\n",
      "Epoch 51/200\n",
      "103/200 [==============>...............] - ETA: 1:28 - loss: 0.0494 - acc: 0.9882"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f551c81c94d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2113\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2114\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2115\u001b[0;31m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    727\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;31m# Make sure to rethrow the first exception in the queue, if any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "#model = keras.applications.xception.Xception(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=num_classes)\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#               optimizer=keras.optimizers.Adam(),\n",
    "#               metrics=['accuracy'])\n",
    "model = load_model('model-xception-0-19-0.98-0.08.h5')\n",
    "checkpoint = ModelCheckpoint('model-xception-1-{epoch:02d}-{val_acc:.2f}-{val_loss:.2f}.h5',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='max')\n",
    "earlystopping = EarlyStopping(monitor='val_acc', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=1e-8, verbose=1)\n",
    "callback_list = [checkpoint, reduce_lr]\n",
    "\n",
    "train_generator = train_data_generator()\n",
    "validation_generator = validation_data_generator()\n",
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=200, \n",
    "                    epochs=epochs, \n",
    "                    callbacks=callback_list,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=20,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(time_taken())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
